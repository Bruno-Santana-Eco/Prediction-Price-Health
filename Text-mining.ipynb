{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bruno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bruno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction given the text.\n",
    "r.extract_keywords_from_text(\"Introduction As you may know now, ChatGPT is a language generation model developed by OpenAI. you can ask it anything you want. So why not learn algorithmic trading with ChatGPT? At the beginning, I wanted to just play with it and see what answers I would get. I didnt believe in it, but I can admit I was really impressed. So let me show you how you could use it to learn different types of algorithmic trading (By practicing using Python code examples). But before that, here are some warnings: There is no recommendation of the best algorithm to use DO NOT USE IT to make investment decisions in financial markets. This is in no way an encouragement to invest in financial markets. Take into account the warning raised by ChatGPT on many code python examples it gives.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Extraction given the list of strings where each string is a sentence.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m sentences\u001b[39m=\u001b[39m [\u001b[39m'''\u001b[39m\u001b[39mIntroduction As you may know now, ChatGPT is a language generation model developed by OpenAI. you can ask it anything you want. \u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[39mSo why not learn algorithmic trading with ChatGPT? At the beginning, I wanted to just play with it and see what answers I would get. \u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[39mI didnt believe in it, but I can admit I was really impressed. So let me show you how you could use it to learn different types of algorithmic \u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39mtrading (By practicing using Python code examples). But before that, here are some warnings: There is no recommendation of the best algorithm\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[39m to use DO NOT USE IT to make investment decisions in financial markets. This is in no way an encouragement to invest in financial markets. \u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39m Take into account the warning raised by ChatGPT on many code python examples it gives.\u001b[39m\u001b[39m'''\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m r\u001b[39m.\u001b[39;49mextract_keywords_from_text(sentences)\u001b[39m# To get keyword phrases ranked highest to lowest.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m r\u001b[39m.\u001b[39mget_ranked_phrases()\n\u001b[0;32m     16\u001b[0m \u001b[39m# To get keyword phrases ranked highest to lowest with scores.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\rake_nltk\\rake.py:126\u001b[0m, in \u001b[0;36mRake.extract_keywords_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_keywords_from_text\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m):\n\u001b[0;32m    122\u001b[0m     \u001b[39m\"\"\"Method to extract keywords from the text provided.\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[39m    :param text: Text to extract keywords from, provided as a string.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     sentences: List[Sentence] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize_text_to_sentences(text)\n\u001b[0;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_keywords_from_sentences(sentences)\n",
      "File \u001b[1;32mc:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\rake_nltk\\rake.py:180\u001b[0m, in \u001b[0;36mRake._tokenize_text_to_sentences\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tokenize_text_to_sentences\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Sentence]:\n\u001b[0;32m    173\u001b[0m     \u001b[39m\"\"\"Tokenizes the given text string into sentences using the configured\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m    sentence tokenizer. Configuration uses `nltk.tokenize.sent_tokenize`\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39m    by default.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39m    :return: List of sentences as per the tokenizer used.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentence_tokenizer(text)\n",
      "File \u001b[1;32mc:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32mc:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[1;32mc:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mc:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    319\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_match_potential_end_contexts(text):\n\u001b[0;32m   1396\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[1;32mc:\\Users\\Bruno\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m before_words \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1374\u001b[0m matches \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 1375\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang_vars\u001b[39m.\u001b[39;49mperiod_context_re()\u001b[39m.\u001b[39;49mfinditer(text))):\n\u001b[0;32m   1376\u001b[0m     \u001b[39m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[39mif\u001b[39;00m matches \u001b[39mand\u001b[39;00m match\u001b[39m.\u001b[39mend() \u001b[39m>\u001b[39m before_start:\n\u001b[0;32m   1378\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Uses stopwords for english from NLTK, and all puntuation characters by\n",
    "# default\n",
    "r = Rake()\n",
    "\n",
    "\n",
    "# Extraction given the list of strings where each string is a sentence.\n",
    "sentences= [\"Introduction As you may know now, ChatGPT is a language generation model developed by OpenAI. you can ask it anything you want. \n",
    "So why not learn algorithmic trading with ChatGPT? At the beginning, I wanted to just play with it and see what answers I would get. \n",
    "I didnt believe in it, but I can admit I was really impressed. So let me show you how you could use it to learn different types of algorithmic \n",
    "trading (By practicing using Python code examples). But before that, here are some warnings: There is no recommendation of the best algorithm\n",
    " to use DO NOT USE IT to make investment decisions in financial markets. This is in no way an encouragement to invest in financial markets. \n",
    " Take into account the warning raised by ChatGPT on many code python examples it gives.\"]\n",
    "r.extract_keywords_from_text(sentences)# To get keyword phrases ranked highest to lowest.\n",
    "r.get_ranked_phrases()\n",
    "\n",
    "# To get keyword phrases ranked highest to lowest with scores.\n",
    "r.get_ranked_phrases_with_scores()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Bruno\\\\PYTHON\\\\PROJETOS\\\\Prediction-Price-Health'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "practicing using python code examples ).\n",
      "many code python examples\n",
      "language generation model developed\n",
      "make investment decisions\n",
      "learn different types\n",
      "learn algorithmic trading\n",
      "algorithmic trading\n",
      "would get\n",
      "warning raised\n",
      "really impressed\n",
      "may know\n",
      "financial markets\n",
      "financial markets\n",
      "didnt believe\n",
      "best algorithm\n",
      "could use\n",
      "use\n",
      "use\n",
      "way\n",
      "warnings\n",
      "wanted\n",
      "want\n",
      "take\n",
      "show\n",
      "see\n",
      "recommendation\n",
      "play\n",
      "openai\n",
      "let\n",
      "invest\n",
      "introduction\n",
      "gives\n",
      "encouragement\n",
      "chatgpt\n",
      "chatgpt\n",
      "chatgpt\n",
      "beginning\n",
      "ask\n",
      "anything\n",
      "answers\n",
      "admit\n",
      "account\n"
     ]
    }
   ],
   "source": [
    "# Ler o texto de um arquivo\n",
    "with open('teste.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Extrair palavras-chave do texto\n",
    "r.extract_keywords_from_text(text)\n",
    "\n",
    "# Obter as frases-chave classificadas do mais alto para o mais baixo\n",
    "key_phrases = r.get_ranked_phrases()\n",
    "\n",
    "# Imprimir as frases-chave\n",
    "for phrase in key_phrases:\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maçã', 'banana', 'areia']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def select_keywords(text, keywords):\n",
    "    selected_words = []\n",
    "    for keyword in keywords:\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(keyword))  # cria um padrão de correspondência para a palavra-chave\n",
    "        matches = re.findall(pattern, text, flags=re.IGNORECASE)  # encontra todas as correspondências no texto\n",
    "        selected_words.extend(matches)  # adiciona as correspondências à lista de palavras selecionadas\n",
    "    return selected_words\n",
    "\n",
    "\n",
    "text = \"Eu preciso de uma maçã, banana, amendoim, água, areia, computador. \"\n",
    "keywords = ['maçã', 'banana','areia']\n",
    "\n",
    "selected_keywords = select_keywords(text, keywords)\n",
    "print(selected_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maçã', 'banana', 'areia']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def select_keywords(text, keywords):\n",
    "    selected_words = []\n",
    "    for keyword in keywords:\n",
    "        pattern = r'\\b{}\\b'.format(re.escape(keyword))  # cria um padrão de correspondência para a palavra-chave\n",
    "        matches = re.findall(pattern, text, flags=re.IGNORECASE)  # encontra todas as correspondências no texto\n",
    "        selected_words.extend(matches)  # adiciona as correspondências à lista de palavras selecionadas\n",
    "    return selected_words\n",
    "\n",
    "\n",
    "with open('teste1.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "keywords = ['maçã', 'banana','areia']\n",
    "\n",
    "selected_keywords = select_keywords(text, keywords)\n",
    "print(selected_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
